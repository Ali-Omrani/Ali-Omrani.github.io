pub_date	title	venue	excerpt	citation	url_slug	paper_url
2022-03-01	Introducing the Gab Hate Corpus: defining and applying hate-based rhetoric to social media posts at scale	Language Resources and Evaluation	The Gab Hate Corpus (GHC) contains 27,665 posts from gab.com, annotated for "hate-based rhetoric" by three or more annotators. It includes hierarchical labels for dehumanizing and violent speech, targeted groups, and rhetorical framing. The GHC enhances existing hate speech datasets with a large, representative collection of richly annotated social media posts	Kennedy, Brendan, et al. "Introducing the Gab Hate Corpus: defining and applying hate-based rhetoric to social media posts at scale." Language Resources and Evaluation (2022): 1-30.	GHC	https://par.nsf.gov/servlets/purl/10322251
2021-07-01	Moral concerns are differentially observable in language	Cognition 	We examined the connection between language usage and moral concerns. We collected a large dataset of Facebook status updates from English-speaking participants, along with their responses on the Moral Foundations Questionnaire. Our findings indicate that individuals' moral concerns can be identified through their language usage, although the strength of this relationship varies across different moral dimensions. 	Kennedy, Brendan, et al. "Moral concerns are differentially observable in language." Cognition 212 (2021): 104696.	FB-cognition	https://par.nsf.gov/servlets/purl/10220769
2021-08-03	Improving counterfactual generation for fair hate speech detection	ACL	Bias mitigation approaches reduce models' dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In hate speech detection, however, equalizing model predictions may ignore important differences among targeted social groups, as hate speech can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among counterfactuals, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.	Davani, Aida Mostafazadeh, et al. "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)." Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021). 2021.	Counterfactual	https://aclanthology.org/2021.woah-1.10/
2023-07-01	Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model	ACL	Existing methods typically rely on word pairs specific to certain social groups, limiting their effectiveness to one aspect of social identity. This approach becomes impractical and costly when addressing bias in lesser-known or unmarked social groups. Instead, we proposed leveraging the Stereotype Content Model (SCM), a framework from social psychology. The SCM categorizes stereotypes along two dimensions: warmth and competence. By adopting this social-group-agnostic perspective, we demonstrated comparable performance to group-specific debiasing methods while offering theoretical and practical advantages over existing techniques.	Ali Omrani, Alireza Salkhordeh Ziabari, Charles Yu, Preni Golazizian, Brendan Kennedy, Mohammad Atari, Heng Ji, and Morteza Dehghani. 2023. Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4123–4139, Toronto, Canada. Association for Computational Linguistics.	SCMD	https://aclanthology.org/2023.acl-long.227.pdf
2023/4/12	The paucity of morality in everyday talk	Scientific Reports	Given its centrality in scholarly and popular discourse, morality should be expected to figure prominently in everyday talk. We test this expectation by examining the frequency of moral content in three contexts, using three methods: (a) Participants’ subjective frequency estimates (N = 581); (b) Human content analysis of unobtrusively recorded in-person interactions (N = 542 participants; n = 50,961 observations); and (c) Computational content analysis of Facebook posts (N = 3822 participants; n = 111,886 observations). In their self-reports, participants estimated that 21.5% of their interactions touched on morality (Study 1), but objectively, only 4.7% of recorded conversational samples (Study 2) and 2.2% of Facebook posts (Study 3) contained moral content. Collectively, these findings suggest that morality may be far less prominent in everyday life than scholarly and popular discourse, and laypeople ...	Atari, Mohammad, Matthias R. Mehl, Jesse Graham, John M. Doris, Norbert Schwarz, Aida Mostafazadeh Davani, Ali Omrani et al. "The paucity of morality in everyday talk." Scientific Reports 13, no. 1 (2023): 5967.	Paucity	https://www.nature.com/articles/s41598-023-32711-4
2023-07-01	The sins of the parents are to be laid upon the children: biased humans, biased data, biased models	Perspectives in Psychological Sciences	Technological innovations have become a key driver of societal advancements. Nowhere is this more evident than in the field of machine learning (ML), which has developed algorithmic models that shape our decisions, behaviors, and outcomes. These tools have widespread use, in part, because they can synthesize massive amounts of data to make seemingly objective recommendations. Yet, in the past few years, the ML community has been raising the alarm on why we should be cautious in interpreting and using these models: they are created by humans, from data generated by humans, whose psychology allows for various biases that impact how the models are developed, trained, tested and interpreted. As psychologists, we thus face a fork in the road; Down the first path, we can continue to use these models without examining and addressing these critical flaws, and rely on computer scientists to try to mitigate them. Down the second path, we can turn our expertise in bias towards this growing field, collaborating with computer scientists to mitigate the deleterious outcomes associated with these models. This paper serves to light the way down the second path by identifying how extant psychological research can help examine and mitigate bias in ML models.	Osborne, Merrick, Ali Omrani, and Morteza Dehghani. "The sins of the parents are to be laid upon the children: biased humans, biased data, biased models." (2022).	PPS	https://psyarxiv.com/4eqnk/download?format=pdf
2022-01-01	The moral foundations reddit corpus	arxiv preprint	Moral framing and sentiment can affect a variety of online and offline behaviors, including donation, pro-environmental action, political engagement, and even participation in violent protests. Various computational methods in Natural Language Processing (NLP) have been used to detect moral sentiment from textual data, but in order to achieve better performances in such subjective tasks, large sets of hand-annotated training data are needed. Previous corpora annotated for moral sentiment have proven valuable, and have generated new insights both within NLP and across the social sciences, but have been limited to Twitter. To facilitate improving our understanding of the role of moral rhetoric, we present the Moral Foundations Reddit Corpus, a collection of 16,123 Reddit comments that have been curated from 12 distinct subreddits, hand-annotated by at least three trained annotators for 8 categories of moral sentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty, Thin Morality, Implicit/Explicit Morality) based on the updated Moral Foundations Theory (MFT) framework. We use a range of methodologies to provide baseline moral-sentiment classification results for this new corpus, e.g., cross-domain classification and knowledge transfer.	Trager, Jackson, Alireza S. Ziabari, Aida Mostafazadeh Davani, Preni Golazazian, Farzan Karimi-Malekabadi, Ali Omrani, Zhihe Li et al. "The moral foundations reddit corpus." arXiv preprint arXiv:2208.05545 (2022).	MFRC	https://arxiv.org/pdf/2208.05545